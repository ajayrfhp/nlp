{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do text classification on IMDB dataset\n",
    "\n",
    "- Block of word model\n",
    "- Naive Bayes\n",
    "- Logistic regression\n",
    "- Get pre-trained vectors from word2vec and train linear classifier by freezing weights\n",
    "- Get pre-trained vectors from BERT, freeze model and re-train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'this': 0, 'is': 1, 'positive': 2, 'amazing': 3, 'negative': 4, 'bad': 5})\n",
      "1 [0.   0.25]\n",
      "1 [0.  0.5]\n",
      "0 [0.25 0.5 ]\n",
      "0 [0.5 0.5]\n",
      "defaultdict(<function NaiveBayes.__init__.<locals>.<lambda> at 0x7f0457942ee0>, {1: array([2., 2., 3., 3., 1., 1.]), 0: array([2., 2., 1., 1., 3., 3.])})\n",
      "[0.75       0.58333333]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab(X):\n",
    "    vocab, word2int = set(), defaultdict(int)\n",
    "    i = 0\n",
    "    for x in X:\n",
    "        for word in x:\n",
    "            if not word in vocab:\n",
    "                vocab.add(word)\n",
    "                word2int[word] = i\n",
    "                i += 1\n",
    "    return vocab, word2int\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, vocab_size, word2int, num_classes):\n",
    "        \"\"\"NaiveBayes model classifier with bag of words to featurize.\n",
    "\n",
    "        Args:\n",
    "            class_labels List[str]: list of class labels\n",
    "            vocab_size int: number of words \n",
    "        \"\"\"\n",
    "        self.num_classes = num_classes\n",
    "        self.prior = np.zeros(num_classes)\n",
    "        self.likelihood = defaultdict(lambda : np.ones(vocab_size))\n",
    "        self.word2int = word2int\n",
    "\n",
    "        pass \n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        for x, y in zip(X, Y):\n",
    "            self.prior[y] += 1 / len(Y)\n",
    "            print(y, self.prior)\n",
    "            for word in x:\n",
    "                self.likelihood[y][self.word2int[word]] += 1\n",
    "        \n",
    "        print(self.likelihood)\n",
    "\n",
    "        for x, y in zip(X, Y):\n",
    "            normalizer = np.sum(self.likelihood[y])\n",
    "            for word in word2int.keys():\n",
    "                self.likelihood[y][self.word2int[word]] /= normalizer\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            probs = np.zeros(self.num_classes)\n",
    "            for c in range(self.num_classes):\n",
    "                probs[c] += self.prior[c]\n",
    "                for word in x:\n",
    "                    probs[c] += self.likelihood[c][self.word2int[word]]\n",
    "            print(probs)\n",
    "            predictions.append(probs.argmax())\n",
    "        return predictions\n",
    "            \n",
    "\n",
    "X = [\n",
    "        ['this', 'is', 'positive', 'amazing'],\n",
    "        ['positive', 'amazing'],\n",
    "        ['this', 'is','negative','bad'],\n",
    "        ['negative','bad']\n",
    "    ]\n",
    "Y = [1, 1, 0, 0]\n",
    "vocab, word2int = get_vocab(X)\n",
    "print(word2int)\n",
    "naive_bayes = NaiveBayes(vocab_size=len(vocab), word2int=word2int, num_classes=2)\n",
    "naive_bayes.fit(X, Y)\n",
    "naive_bayes.predict([['bad']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.NaiveBayes.__init__.<locals>.<lambda>()>,\n",
       "            {1: array([0.16666667, 0.16666667, 0.25      , 0.25      , 0.08333333,\n",
       "                    0.08333333]),\n",
       "             0: array([0.16666667, 0.16666667, 0.08333333, 0.08333333, 0.25      ,\n",
       "                    0.25      ])})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('reddevils_nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "207f4bc9e0c83dc0c8ddefb60bf2781db591dfbb838789bb02122c3833b8e815"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
