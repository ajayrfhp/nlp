{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, Trainer\n",
    "import datasets\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from datasets import load_dataset, get_dataset_split_names\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train language model in wikipedia dataset\n",
    "- Visualize embeddings for a few words\n",
    "- Freeze embeddings and train linear model on top of embeddings and do IMDB classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikitext/wikitext-103-v1 (download: 181.42 MiB, generated: 522.23 MiB, post-processed: Unknown size, total: 703.64 MiB) to /home/ajrfhp/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b520a8789f4501a55e7ac596ed4376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/190M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73444bbf4afa4aa393038024517a21af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee6ace8aeeb44cb84404b49f0ca9f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dec177ec2fc4deab4e833b47bbf1935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikitext downloaded and prepared to /home/ajrfhp/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/home/ajrfhp/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\",\n",
       " 1801350)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"wikitext\", 'wikitext-103-v1', split=\"train\")\n",
    "validation_dataset = load_dataset(\"wikitext\", 'wikitext-103-v1', split=\"validation\")\n",
    "toy_text = \"This is a sentence\"\n",
    "\n",
    "train_dataset['text'][5], len(train_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokens):\n",
    "    return tokenizer.batch_encode_plus(tokens[\"text\"])\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83617a18e39340d99a3b71dd07d235c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/451 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51666d0631a84078b0090c0ed8f94c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/451 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d57747762c466993962ffad00ba1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/451 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8042d671cb944dc18078d0c5f760d6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/451 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (688 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5562f13046264d9a91172824883e957e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a9dc72aea54bad9c3a9d06a3ad9374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09db1fe854a441dab716b871221db697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff2c81d7ea34e5a94c9a1c0033c16e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "tokenized_validation_dataset = validation_dataset.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86097d561e84456a0f86aa373f8c893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_skipgram_context(batch_text, negative_sampling_rate=5, window_size=3, max_samples = 100000):\n",
    "    X, Y = [], []\n",
    "    batch_ids = batch_text['input_ids']\n",
    "    for sentence in batch_ids:\n",
    "        if len(X) > max_samples:\n",
    "            break\n",
    "        for i, word in enumerate(sentence):\n",
    "            for j in range(i-window_size,i+window_size+1):\n",
    "                if j > 0 and j < len(sentence) and i != j:\n",
    "                    X.append((sentence[i], sentence[j]))\n",
    "                    Y.append(1)\n",
    "                    for k in range(negative_sampling_rate):\n",
    "                        negative_sample = np.random.randint(tokenizer.vocab_size)\n",
    "                        if sentence[i] != negative_sample:\n",
    "                            X.append((sentence[i], negative_sample))\n",
    "                            Y.append(0)\n",
    "    X = np.array(X).reshape((-1, 2))\n",
    "    Y = np.array(Y)\n",
    "    indices = np.random.permutation(len(X))\n",
    "    return { \"inputs\" : X[indices], \"outputs\" : Y[indices]}\n",
    "    \n",
    "    \n",
    "skipgram_train_dataset = tokenized_train_dataset.map(prepare_skipgram_context, batched=True, remove_columns=['input_ids','token_type_ids','attention_mask'])\n",
    "skipgram_train_dataset.set_format(\"torch\")\n",
    "\n",
    "skipgram_validation_dataset = tokenized_validation_dataset.map(prepare_skipgram_context, batched=True, remove_columns=['input_ids','token_type_ids','attention_mask'])\n",
    "skipgram_validation_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    skipgram_train_dataset, shuffle=True, batch_size=2000,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    skipgram_validation_dataset, batch_size=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8225, 0.1322, 0.3112], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WordToVec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim) -> None:\n",
    "        super(WordToVec, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        v = self.embedding(x[:,0])\n",
    "        u = self.context(x[:,1])\n",
    "        score = torch.sum(u * v, dim=-1)\n",
    "        return nn.Sigmoid()(score)\n",
    "\n",
    "word_to_vec = WordToVec(tokenizer.vocab_size, 100)\n",
    "random_input = torch.tensor((\n",
    "    [10, 5],\n",
    "    [7, 3],\n",
    "    [9, 2]\n",
    "))\n",
    "word_to_vec(random_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.505\n",
      "epoch 0, number of batches processed 100 of 3821, training_loss = 7.247525353431701\n",
      "epoch 0, number of batches processed 200 of 3821, training_loss = 6.939441113471985\n",
      "epoch 0, number of batches processed 300 of 3821, training_loss = 6.74136513710022\n",
      "epoch 0, number of batches processed 400 of 3821, training_loss = 6.602007741928101\n",
      "epoch 0, number of batches processed 500 of 3821, training_loss = 6.479649324417114\n",
      "epoch 0, number of batches processed 600 of 3821, training_loss = 6.25370918750763\n",
      "epoch 0, number of batches processed 700 of 3821, training_loss = 6.176643567085266\n",
      "epoch 0, number of batches processed 800 of 3821, training_loss = 5.959899754524231\n",
      "epoch 0, number of batches processed 900 of 3821, training_loss = 5.885436158180237\n",
      "epoch 0, number of batches processed 1000 of 3821, training_loss = 5.7955278968811035\n",
      "epoch 0, number of batches processed 1100 of 3821, training_loss = 5.495901441574096\n",
      "epoch 0, number of batches processed 1200 of 3821, training_loss = 5.620334105491638\n",
      "epoch 0, number of batches processed 1300 of 3821, training_loss = 5.309284768104553\n",
      "epoch 0, number of batches processed 1400 of 3821, training_loss = 5.2076415967941285\n",
      "epoch 0, number of batches processed 1500 of 3821, training_loss = 5.075762100219727\n",
      "epoch 0, number of batches processed 1600 of 3821, training_loss = 4.979161312580109\n",
      "epoch 0, number of batches processed 1700 of 3821, training_loss = 4.934087529182434\n",
      "epoch 0, number of batches processed 1800 of 3821, training_loss = 4.81772540807724\n",
      "epoch 0, number of batches processed 1900 of 3821, training_loss = 4.789564278125763\n",
      "epoch 0, number of batches processed 2000 of 3821, training_loss = 4.656745338439942\n",
      "epoch 0, number of batches processed 2100 of 3821, training_loss = 4.484692928791046\n",
      "epoch 0, number of batches processed 2200 of 3821, training_loss = 4.497847008705139\n",
      "epoch 0, number of batches processed 2300 of 3821, training_loss = 4.422509183883667\n",
      "epoch 0, number of batches processed 2400 of 3821, training_loss = 4.383432159423828\n",
      "epoch 0, number of batches processed 2500 of 3821, training_loss = 4.351129071712494\n",
      "epoch 0, number of batches processed 2600 of 3821, training_loss = 4.274367983341217\n",
      "epoch 0, number of batches processed 2700 of 3821, training_loss = 4.0509629464149475\n",
      "epoch 0, number of batches processed 2800 of 3821, training_loss = 4.17969774723053\n",
      "epoch 0, number of batches processed 2900 of 3821, training_loss = 4.055576090812683\n",
      "epoch 0, number of batches processed 3000 of 3821, training_loss = 3.9369789075851442\n",
      "epoch 0, number of batches processed 3100 of 3821, training_loss = 3.9679586815834047\n",
      "epoch 0, number of batches processed 3200 of 3821, training_loss = 3.9599855184555053\n",
      "epoch 0, number of batches processed 3300 of 3821, training_loss = 3.845408544540405\n",
      "epoch 0, number of batches processed 3400 of 3821, training_loss = 3.817521529197693\n",
      "epoch 0, number of batches processed 3500 of 3821, training_loss = 3.8104249262809753\n",
      "epoch 0, number of batches processed 3600 of 3821, training_loss = 3.719287886619568\n",
      "epoch 0, number of batches processed 3700 of 3821, training_loss = 3.637950432300568\n",
      "epoch 0, number of batches processed 3800 of 3821, training_loss = 3.4929286432266236\n",
      "0 0.556\n",
      "acc epoch0 0.5608333333333333\n",
      "epoch 1, number of batches processed 100 of 3821, training_loss = 3.3383466577529908\n",
      "epoch 1, number of batches processed 200 of 3821, training_loss = 3.1915441656112673\n",
      "epoch 1, number of batches processed 300 of 3821, training_loss = 3.132244930267334\n",
      "epoch 1, number of batches processed 400 of 3821, training_loss = 3.0950465965270997\n",
      "epoch 1, number of batches processed 500 of 3821, training_loss = 3.089095804691315\n",
      "epoch 1, number of batches processed 600 of 3821, training_loss = 3.077697994709015\n",
      "epoch 1, number of batches processed 700 of 3821, training_loss = 3.0685271072387694\n",
      "epoch 1, number of batches processed 800 of 3821, training_loss = 3.0204051899909974\n",
      "epoch 1, number of batches processed 900 of 3821, training_loss = 2.961298363208771\n",
      "epoch 1, number of batches processed 1000 of 3821, training_loss = 2.924215533733368\n",
      "epoch 1, number of batches processed 1100 of 3821, training_loss = 2.9990888559818267\n",
      "epoch 1, number of batches processed 1200 of 3821, training_loss = 2.9360468125343324\n",
      "epoch 1, number of batches processed 1300 of 3821, training_loss = 2.8364464163780214\n",
      "epoch 1, number of batches processed 1400 of 3821, training_loss = 2.8756493616104124\n",
      "epoch 1, number of batches processed 1500 of 3821, training_loss = 2.805963599681854\n",
      "epoch 1, number of batches processed 1600 of 3821, training_loss = 2.841798654794693\n",
      "epoch 1, number of batches processed 1700 of 3821, training_loss = 2.7163905835151674\n",
      "epoch 1, number of batches processed 1800 of 3821, training_loss = 2.7521242189407347\n",
      "epoch 1, number of batches processed 1900 of 3821, training_loss = 2.6723533165454865\n",
      "epoch 1, number of batches processed 2000 of 3821, training_loss = 2.6508740854263304\n",
      "epoch 1, number of batches processed 2100 of 3821, training_loss = 2.7377654409408567\n",
      "epoch 1, number of batches processed 2200 of 3821, training_loss = 2.657859152555466\n",
      "epoch 1, number of batches processed 2300 of 3821, training_loss = 2.5690793418884277\n",
      "epoch 1, number of batches processed 2400 of 3821, training_loss = 2.5563929200172426\n",
      "epoch 1, number of batches processed 2500 of 3821, training_loss = 2.595041652917862\n",
      "epoch 1, number of batches processed 2600 of 3821, training_loss = 2.540175164937973\n",
      "epoch 1, number of batches processed 2700 of 3821, training_loss = 2.481540312767029\n",
      "epoch 1, number of batches processed 2800 of 3821, training_loss = 2.442919373512268\n",
      "epoch 1, number of batches processed 2900 of 3821, training_loss = 2.442975604534149\n",
      "epoch 1, number of batches processed 3000 of 3821, training_loss = 2.4310927283763886\n",
      "epoch 1, number of batches processed 3100 of 3821, training_loss = 2.4198212277889253\n",
      "epoch 1, number of batches processed 3200 of 3821, training_loss = 2.336668539047241\n",
      "epoch 1, number of batches processed 3300 of 3821, training_loss = 2.3139244890213013\n",
      "epoch 1, number of batches processed 3400 of 3821, training_loss = 2.288762925863266\n",
      "epoch 1, number of batches processed 3500 of 3821, training_loss = 2.240070515871048\n",
      "epoch 1, number of batches processed 3600 of 3821, training_loss = 2.2960296094417574\n",
      "epoch 1, number of batches processed 3700 of 3821, training_loss = 2.1744319117069244\n",
      "epoch 1, number of batches processed 3800 of 3821, training_loss = 2.206367577314377\n",
      "0 0.673\n",
      "acc epoch1 0.6870833333333334\n",
      "epoch 2, number of batches processed 100 of 3821, training_loss = 1.9531465399265289\n",
      "epoch 2, number of batches processed 200 of 3821, training_loss = 1.9165009701251983\n",
      "epoch 2, number of batches processed 300 of 3821, training_loss = 1.8930509197711944\n",
      "epoch 2, number of batches processed 400 of 3821, training_loss = 1.8819083845615387\n",
      "epoch 2, number of batches processed 500 of 3821, training_loss = 1.8922373795509337\n",
      "epoch 2, number of batches processed 600 of 3821, training_loss = 1.8733499109745027\n",
      "epoch 2, number of batches processed 700 of 3821, training_loss = 1.8157238149642945\n",
      "epoch 2, number of batches processed 800 of 3821, training_loss = 1.803470311164856\n",
      "epoch 2, number of batches processed 900 of 3821, training_loss = 1.7706009602546693\n",
      "epoch 2, number of batches processed 1000 of 3821, training_loss = 1.7646048605442046\n",
      "epoch 2, number of batches processed 1100 of 3821, training_loss = 1.7434548902511597\n",
      "epoch 2, number of batches processed 1200 of 3821, training_loss = 1.7586478543281556\n",
      "epoch 2, number of batches processed 1300 of 3821, training_loss = 1.7227932655811309\n",
      "epoch 2, number of batches processed 1400 of 3821, training_loss = 1.6875526762008668\n",
      "epoch 2, number of batches processed 1500 of 3821, training_loss = 1.6752029579877854\n",
      "epoch 2, number of batches processed 1600 of 3821, training_loss = 1.6682925581932069\n",
      "epoch 2, number of batches processed 1700 of 3821, training_loss = 1.6241774582862853\n",
      "epoch 2, number of batches processed 1800 of 3821, training_loss = 1.5784670972824097\n",
      "epoch 2, number of batches processed 1900 of 3821, training_loss = 1.566434725522995\n",
      "epoch 2, number of batches processed 2000 of 3821, training_loss = 1.5754042613506316\n",
      "epoch 2, number of batches processed 2100 of 3821, training_loss = 1.5187277847528458\n",
      "epoch 2, number of batches processed 2200 of 3821, training_loss = 1.5808169829845429\n",
      "epoch 2, number of batches processed 2300 of 3821, training_loss = 1.539533606171608\n",
      "epoch 2, number of batches processed 2400 of 3821, training_loss = 1.5177952980995177\n",
      "epoch 2, number of batches processed 2500 of 3821, training_loss = 1.5074213355779649\n",
      "epoch 2, number of batches processed 2600 of 3821, training_loss = 1.475405204296112\n",
      "epoch 2, number of batches processed 2700 of 3821, training_loss = 1.4711378741264343\n",
      "epoch 2, number of batches processed 2800 of 3821, training_loss = 1.4853661531209945\n",
      "epoch 2, number of batches processed 2900 of 3821, training_loss = 1.428663576245308\n",
      "epoch 2, number of batches processed 3000 of 3821, training_loss = 1.3853363865613937\n",
      "epoch 2, number of batches processed 3100 of 3821, training_loss = 1.3868692046403885\n",
      "epoch 2, number of batches processed 3200 of 3821, training_loss = 1.3645698952674865\n",
      "epoch 2, number of batches processed 3300 of 3821, training_loss = 1.3464309525489808\n",
      "epoch 2, number of batches processed 3400 of 3821, training_loss = 1.3513208466768265\n",
      "epoch 2, number of batches processed 3500 of 3821, training_loss = 1.3360134571790696\n",
      "epoch 2, number of batches processed 3600 of 3821, training_loss = 1.3364305663108826\n",
      "epoch 2, number of batches processed 3700 of 3821, training_loss = 1.2949042850732804\n",
      "epoch 2, number of batches processed 3800 of 3821, training_loss = 1.3310159635543823\n",
      "0 0.759\n",
      "acc epoch2 0.7660833333333333\n",
      "epoch 3, number of batches processed 100 of 3821, training_loss = 1.1168106561899185\n",
      "epoch 3, number of batches processed 200 of 3821, training_loss = 1.0970355921983719\n",
      "epoch 3, number of batches processed 300 of 3821, training_loss = 1.0984208077192306\n",
      "epoch 3, number of batches processed 400 of 3821, training_loss = 1.0858632469177245\n",
      "epoch 3, number of batches processed 500 of 3821, training_loss = 1.0672015869617462\n",
      "epoch 3, number of batches processed 600 of 3821, training_loss = 1.08624784886837\n",
      "epoch 3, number of batches processed 700 of 3821, training_loss = 1.0177191996574402\n",
      "epoch 3, number of batches processed 800 of 3821, training_loss = 1.0568117505311967\n",
      "epoch 3, number of batches processed 900 of 3821, training_loss = 1.0381686460971833\n",
      "epoch 3, number of batches processed 1000 of 3821, training_loss = 1.0090870428085328\n",
      "epoch 3, number of batches processed 1100 of 3821, training_loss = 1.006078787446022\n",
      "epoch 3, number of batches processed 1200 of 3821, training_loss = 1.0033149552345275\n",
      "epoch 3, number of batches processed 1300 of 3821, training_loss = 0.9901159751415253\n",
      "epoch 3, number of batches processed 1400 of 3821, training_loss = 1.0077583581209182\n",
      "epoch 3, number of batches processed 1500 of 3821, training_loss = 0.9566943037509918\n",
      "epoch 3, number of batches processed 1600 of 3821, training_loss = 0.9985070729255676\n",
      "epoch 3, number of batches processed 1700 of 3821, training_loss = 0.9865198904275894\n",
      "epoch 3, number of batches processed 1800 of 3821, training_loss = 0.9492759054899216\n",
      "epoch 3, number of batches processed 1900 of 3821, training_loss = 0.9414926391839981\n",
      "epoch 3, number of batches processed 2000 of 3821, training_loss = 0.9309166252613068\n",
      "epoch 3, number of batches processed 2100 of 3821, training_loss = 0.9156273931264878\n",
      "epoch 3, number of batches processed 2200 of 3821, training_loss = 0.8801221024990081\n",
      "epoch 3, number of batches processed 2300 of 3821, training_loss = 0.9002211606502533\n",
      "epoch 3, number of batches processed 2400 of 3821, training_loss = 0.9071608677506446\n",
      "epoch 3, number of batches processed 2500 of 3821, training_loss = 0.9390694725513459\n",
      "epoch 3, number of batches processed 2600 of 3821, training_loss = 0.9059331953525543\n",
      "epoch 3, number of batches processed 2700 of 3821, training_loss = 0.890169233083725\n",
      "epoch 3, number of batches processed 2800 of 3821, training_loss = 0.8677564555406571\n",
      "epoch 3, number of batches processed 2900 of 3821, training_loss = 0.8669341012835503\n",
      "epoch 3, number of batches processed 3000 of 3821, training_loss = 0.8657712432742118\n",
      "epoch 3, number of batches processed 3100 of 3821, training_loss = 0.8974611419439316\n",
      "epoch 3, number of batches processed 3200 of 3821, training_loss = 0.7976277834177017\n",
      "epoch 3, number of batches processed 3300 of 3821, training_loss = 0.8458208754658699\n",
      "epoch 3, number of batches processed 3400 of 3821, training_loss = 0.8146666157245636\n",
      "epoch 3, number of batches processed 3500 of 3821, training_loss = 0.824042900800705\n",
      "epoch 3, number of batches processed 3600 of 3821, training_loss = 0.8571872952580452\n",
      "epoch 3, number of batches processed 3700 of 3821, training_loss = 0.8263010308146477\n",
      "epoch 3, number of batches processed 3800 of 3821, training_loss = 0.8209784635901451\n",
      "0 0.79\n",
      "acc epoch3 0.797\n",
      "epoch 4, number of batches processed 100 of 3821, training_loss = 0.7218409025669098\n",
      "epoch 4, number of batches processed 200 of 3821, training_loss = 0.6731370359659195\n",
      "epoch 4, number of batches processed 300 of 3821, training_loss = 0.6745993310213089\n",
      "epoch 4, number of batches processed 400 of 3821, training_loss = 0.6328340998291969\n",
      "epoch 4, number of batches processed 500 of 3821, training_loss = 0.6724670702219009\n",
      "epoch 4, number of batches processed 600 of 3821, training_loss = 0.6599451869726181\n",
      "epoch 4, number of batches processed 700 of 3821, training_loss = 0.6162607702612877\n",
      "epoch 4, number of batches processed 800 of 3821, training_loss = 0.6338865560293198\n",
      "epoch 4, number of batches processed 900 of 3821, training_loss = 0.6432528564333916\n",
      "epoch 4, number of batches processed 1000 of 3821, training_loss = 0.6534428477287293\n",
      "epoch 4, number of batches processed 1100 of 3821, training_loss = 0.6646059331297874\n",
      "epoch 4, number of batches processed 1200 of 3821, training_loss = 0.6263553017377853\n",
      "epoch 4, number of batches processed 1300 of 3821, training_loss = 0.6355567872524261\n",
      "epoch 4, number of batches processed 1400 of 3821, training_loss = 0.6407556056976318\n",
      "epoch 4, number of batches processed 1500 of 3821, training_loss = 0.6193968021869659\n",
      "epoch 4, number of batches processed 1600 of 3821, training_loss = 0.5870956352353096\n",
      "epoch 4, number of batches processed 1700 of 3821, training_loss = 0.6175864443182946\n",
      "epoch 4, number of batches processed 1800 of 3821, training_loss = 0.6079340609908104\n",
      "epoch 4, number of batches processed 1900 of 3821, training_loss = 0.6285404893755913\n",
      "epoch 4, number of batches processed 2000 of 3821, training_loss = 0.6041768908500671\n",
      "epoch 4, number of batches processed 2100 of 3821, training_loss = 0.5739614075422287\n",
      "epoch 4, number of batches processed 2200 of 3821, training_loss = 0.59970348238945\n",
      "epoch 4, number of batches processed 2300 of 3821, training_loss = 0.6040534713864326\n",
      "epoch 4, number of batches processed 2400 of 3821, training_loss = 0.5749993720650672\n",
      "epoch 4, number of batches processed 2500 of 3821, training_loss = 0.5851249352097512\n",
      "epoch 4, number of batches processed 2600 of 3821, training_loss = 0.6052005091309547\n",
      "epoch 4, number of batches processed 2700 of 3821, training_loss = 0.5575106695294381\n",
      "epoch 4, number of batches processed 2800 of 3821, training_loss = 0.5932447269558907\n",
      "epoch 4, number of batches processed 2900 of 3821, training_loss = 0.6181032478809356\n",
      "epoch 4, number of batches processed 3000 of 3821, training_loss = 0.5793485024571419\n",
      "epoch 4, number of batches processed 3100 of 3821, training_loss = 0.5455887722969055\n",
      "epoch 4, number of batches processed 3200 of 3821, training_loss = 0.5995691373944283\n",
      "epoch 4, number of batches processed 3300 of 3821, training_loss = 0.5677194213867187\n",
      "epoch 4, number of batches processed 3400 of 3821, training_loss = 0.5621044310927391\n",
      "epoch 4, number of batches processed 3500 of 3821, training_loss = 0.524218778014183\n",
      "epoch 4, number of batches processed 3600 of 3821, training_loss = 0.5565536065399647\n",
      "epoch 4, number of batches processed 3700 of 3821, training_loss = 0.5228569224476814\n",
      "epoch 4, number of batches processed 3800 of 3821, training_loss = 0.5415441998839379\n",
      "0 0.807\n",
      "acc epoch4 0.8160833333333334\n",
      "epoch 5, number of batches processed 100 of 3821, training_loss = 0.46439491257071497\n",
      "epoch 5, number of batches processed 200 of 3821, training_loss = 0.47758293986320494\n",
      "epoch 5, number of batches processed 300 of 3821, training_loss = 0.45897352799773217\n",
      "epoch 5, number of batches processed 400 of 3821, training_loss = 0.4789893680810928\n",
      "epoch 5, number of batches processed 500 of 3821, training_loss = 0.43148267433047294\n",
      "epoch 5, number of batches processed 600 of 3821, training_loss = 0.46696886867284776\n",
      "epoch 5, number of batches processed 700 of 3821, training_loss = 0.43884736254811285\n",
      "epoch 5, number of batches processed 800 of 3821, training_loss = 0.4508193935453892\n",
      "epoch 5, number of batches processed 900 of 3821, training_loss = 0.45103581294417383\n",
      "epoch 5, number of batches processed 1000 of 3821, training_loss = 0.44743501886725423\n",
      "epoch 5, number of batches processed 1100 of 3821, training_loss = 0.39830356165766717\n",
      "epoch 5, number of batches processed 1200 of 3821, training_loss = 0.4417996270954609\n",
      "epoch 5, number of batches processed 1300 of 3821, training_loss = 0.43522351935505865\n",
      "epoch 5, number of batches processed 1400 of 3821, training_loss = 0.42228057846426964\n",
      "epoch 5, number of batches processed 1500 of 3821, training_loss = 0.4369662629067898\n",
      "epoch 5, number of batches processed 1600 of 3821, training_loss = 0.41121342793107035\n",
      "epoch 5, number of batches processed 1700 of 3821, training_loss = 0.4389283533394337\n",
      "epoch 5, number of batches processed 1800 of 3821, training_loss = 0.4374881847202778\n",
      "epoch 5, number of batches processed 1900 of 3821, training_loss = 0.4418946740031242\n",
      "epoch 5, number of batches processed 2000 of 3821, training_loss = 0.4342731037735939\n",
      "epoch 5, number of batches processed 2100 of 3821, training_loss = 0.4108980640769005\n",
      "epoch 5, number of batches processed 2200 of 3821, training_loss = 0.41926729813218117\n",
      "epoch 5, number of batches processed 2300 of 3821, training_loss = 0.4229213774204254\n",
      "epoch 5, number of batches processed 2400 of 3821, training_loss = 0.3962942656874657\n",
      "epoch 5, number of batches processed 2500 of 3821, training_loss = 0.4120084334909916\n",
      "epoch 5, number of batches processed 2600 of 3821, training_loss = 0.41840957328677175\n",
      "epoch 5, number of batches processed 2700 of 3821, training_loss = 0.4061079673469067\n",
      "epoch 5, number of batches processed 2800 of 3821, training_loss = 0.40296453550457956\n",
      "epoch 5, number of batches processed 2900 of 3821, training_loss = 0.41382402017712594\n",
      "epoch 5, number of batches processed 3000 of 3821, training_loss = 0.40378201827406884\n",
      "epoch 5, number of batches processed 3100 of 3821, training_loss = 0.42015538692474363\n",
      "epoch 5, number of batches processed 3200 of 3821, training_loss = 0.413958981782198\n",
      "epoch 5, number of batches processed 3300 of 3821, training_loss = 0.3909957154095173\n",
      "epoch 5, number of batches processed 3400 of 3821, training_loss = 0.415964595079422\n",
      "epoch 5, number of batches processed 3500 of 3821, training_loss = 0.4126945132017136\n",
      "epoch 5, number of batches processed 3600 of 3821, training_loss = 0.3817848853766918\n",
      "epoch 5, number of batches processed 3700 of 3821, training_loss = 0.3851413437724113\n",
      "epoch 5, number of batches processed 3800 of 3821, training_loss = 0.41293236657977106\n",
      "0 0.83\n",
      "acc epoch5 0.8281666666666667\n",
      "epoch 6, number of batches processed 100 of 3821, training_loss = 0.3304836803674698\n",
      "epoch 6, number of batches processed 200 of 3821, training_loss = 0.3340591052174568\n",
      "epoch 6, number of batches processed 300 of 3821, training_loss = 0.3399322174489498\n",
      "epoch 6, number of batches processed 400 of 3821, training_loss = 0.3333125764131546\n",
      "epoch 6, number of batches processed 500 of 3821, training_loss = 0.3264833188056946\n",
      "epoch 6, number of batches processed 600 of 3821, training_loss = 0.333943110704422\n",
      "epoch 6, number of batches processed 700 of 3821, training_loss = 0.3408575576543808\n",
      "epoch 6, number of batches processed 800 of 3821, training_loss = 0.321255099773407\n",
      "epoch 6, number of batches processed 900 of 3821, training_loss = 0.31514639645814896\n",
      "epoch 6, number of batches processed 1000 of 3821, training_loss = 0.3379562459886074\n",
      "epoch 6, number of batches processed 1100 of 3821, training_loss = 0.3456062461435795\n",
      "epoch 6, number of batches processed 1200 of 3821, training_loss = 0.32991434961557387\n",
      "epoch 6, number of batches processed 1300 of 3821, training_loss = 0.32836875587701797\n",
      "epoch 6, number of batches processed 1400 of 3821, training_loss = 0.3208632081747055\n",
      "epoch 6, number of batches processed 1500 of 3821, training_loss = 0.3404413352906704\n",
      "epoch 6, number of batches processed 1600 of 3821, training_loss = 0.32296438872814176\n",
      "epoch 6, number of batches processed 1700 of 3821, training_loss = 0.3185854807496071\n",
      "epoch 6, number of batches processed 1800 of 3821, training_loss = 0.33243352591991426\n",
      "epoch 6, number of batches processed 1900 of 3821, training_loss = 0.3231885442137718\n",
      "epoch 6, number of batches processed 2000 of 3821, training_loss = 0.3205546970665455\n",
      "epoch 6, number of batches processed 2100 of 3821, training_loss = 0.3365538534522057\n",
      "epoch 6, number of batches processed 2200 of 3821, training_loss = 0.31925616517663\n",
      "epoch 6, number of batches processed 2300 of 3821, training_loss = 0.3304112616181374\n",
      "epoch 6, number of batches processed 2400 of 3821, training_loss = 0.3121755762398243\n",
      "epoch 6, number of batches processed 2500 of 3821, training_loss = 0.3311658434569836\n",
      "epoch 6, number of batches processed 2600 of 3821, training_loss = 0.3311491871625185\n",
      "epoch 6, number of batches processed 2700 of 3821, training_loss = 0.320200359672308\n",
      "epoch 6, number of batches processed 2800 of 3821, training_loss = 0.3389218752086163\n",
      "epoch 6, number of batches processed 2900 of 3821, training_loss = 0.30803128346800807\n",
      "epoch 6, number of batches processed 3000 of 3821, training_loss = 0.30123991549015044\n",
      "epoch 6, number of batches processed 3100 of 3821, training_loss = 0.32013469383120535\n",
      "epoch 6, number of batches processed 3200 of 3821, training_loss = 0.34696447908878325\n",
      "epoch 6, number of batches processed 3300 of 3821, training_loss = 0.32426907137036326\n",
      "epoch 6, number of batches processed 3400 of 3821, training_loss = 0.31120930284261705\n",
      "epoch 6, number of batches processed 3500 of 3821, training_loss = 0.31962292566895484\n",
      "epoch 6, number of batches processed 3600 of 3821, training_loss = 0.3197282882034779\n",
      "epoch 6, number of batches processed 3700 of 3821, training_loss = 0.3003966000676155\n",
      "epoch 6, number of batches processed 3800 of 3821, training_loss = 0.32454108029603956\n",
      "0 0.834\n",
      "acc epoch6 0.8363333333333334\n",
      "epoch 7, number of batches processed 100 of 3821, training_loss = 0.2774401966482401\n",
      "epoch 7, number of batches processed 200 of 3821, training_loss = 0.2585321809351444\n",
      "epoch 7, number of batches processed 300 of 3821, training_loss = 0.27411758080124854\n",
      "epoch 7, number of batches processed 400 of 3821, training_loss = 0.2781946965306997\n",
      "epoch 7, number of batches processed 500 of 3821, training_loss = 0.2609993662685156\n",
      "epoch 7, number of batches processed 600 of 3821, training_loss = 0.27231565542519093\n",
      "epoch 7, number of batches processed 700 of 3821, training_loss = 0.2575232729315758\n",
      "epoch 7, number of batches processed 800 of 3821, training_loss = 0.25890747919678686\n",
      "epoch 7, number of batches processed 900 of 3821, training_loss = 0.2568163651227951\n",
      "epoch 7, number of batches processed 1000 of 3821, training_loss = 0.2668512274324894\n",
      "epoch 7, number of batches processed 1100 of 3821, training_loss = 0.2635895308107138\n",
      "epoch 7, number of batches processed 1200 of 3821, training_loss = 0.24951575607061385\n",
      "epoch 7, number of batches processed 1300 of 3821, training_loss = 0.26940909899771215\n",
      "epoch 7, number of batches processed 1400 of 3821, training_loss = 0.2882515747845173\n",
      "epoch 7, number of batches processed 1500 of 3821, training_loss = 0.27721357956528664\n",
      "epoch 7, number of batches processed 1600 of 3821, training_loss = 0.2610586663335562\n",
      "epoch 7, number of batches processed 1700 of 3821, training_loss = 0.2600747977942228\n",
      "epoch 7, number of batches processed 1800 of 3821, training_loss = 0.27804513953626153\n",
      "epoch 7, number of batches processed 1900 of 3821, training_loss = 0.25322702214121817\n",
      "epoch 7, number of batches processed 2000 of 3821, training_loss = 0.2543419986218214\n",
      "epoch 7, number of batches processed 2100 of 3821, training_loss = 0.25351110599935056\n",
      "epoch 7, number of batches processed 2200 of 3821, training_loss = 0.2518117166310549\n",
      "epoch 7, number of batches processed 2300 of 3821, training_loss = 0.26525841422379015\n",
      "epoch 7, number of batches processed 2400 of 3821, training_loss = 0.2950481191277504\n",
      "epoch 7, number of batches processed 2500 of 3821, training_loss = 0.25658207342028616\n",
      "epoch 7, number of batches processed 2600 of 3821, training_loss = 0.26186545968055724\n",
      "epoch 7, number of batches processed 2700 of 3821, training_loss = 0.2785225297510624\n",
      "epoch 7, number of batches processed 2800 of 3821, training_loss = 0.25191441886127\n",
      "epoch 7, number of batches processed 2900 of 3821, training_loss = 0.2627800575643778\n",
      "epoch 7, number of batches processed 3000 of 3821, training_loss = 0.2776061762869358\n",
      "epoch 7, number of batches processed 3100 of 3821, training_loss = 0.248118497133255\n",
      "epoch 7, number of batches processed 3200 of 3821, training_loss = 0.24159047998487948\n",
      "epoch 7, number of batches processed 3300 of 3821, training_loss = 0.2477179028093815\n",
      "epoch 7, number of batches processed 3400 of 3821, training_loss = 0.25890580780804157\n",
      "epoch 7, number of batches processed 3500 of 3821, training_loss = 0.2678160344809294\n",
      "epoch 7, number of batches processed 3600 of 3821, training_loss = 0.272439643740654\n",
      "epoch 7, number of batches processed 3700 of 3821, training_loss = 0.25851851888000965\n",
      "epoch 7, number of batches processed 3800 of 3821, training_loss = 0.25085471853613855\n",
      "0 0.838\n",
      "acc epoch7 0.8419166666666666\n",
      "epoch 8, number of batches processed 100 of 3821, training_loss = 0.2155972733348608\n",
      "epoch 8, number of batches processed 200 of 3821, training_loss = 0.22955397211015224\n",
      "epoch 8, number of batches processed 300 of 3821, training_loss = 0.228103072270751\n",
      "epoch 8, number of batches processed 400 of 3821, training_loss = 0.22271726422011853\n",
      "epoch 8, number of batches processed 500 of 3821, training_loss = 0.23728713050484657\n",
      "epoch 8, number of batches processed 600 of 3821, training_loss = 0.22007319673895837\n",
      "epoch 8, number of batches processed 700 of 3821, training_loss = 0.20735677495598792\n",
      "epoch 8, number of batches processed 800 of 3821, training_loss = 0.21403019219636918\n",
      "epoch 8, number of batches processed 900 of 3821, training_loss = 0.2226304043084383\n",
      "epoch 8, number of batches processed 1000 of 3821, training_loss = 0.2040301364660263\n",
      "epoch 8, number of batches processed 1100 of 3821, training_loss = 0.23360503137111663\n",
      "epoch 8, number of batches processed 1200 of 3821, training_loss = 0.20504246711730956\n",
      "epoch 8, number of batches processed 1300 of 3821, training_loss = 0.2356957446038723\n",
      "epoch 8, number of batches processed 1400 of 3821, training_loss = 0.2161800403892994\n",
      "epoch 8, number of batches processed 1500 of 3821, training_loss = 0.21919606491923332\n",
      "epoch 8, number of batches processed 1600 of 3821, training_loss = 0.22624251380562782\n",
      "epoch 8, number of batches processed 1700 of 3821, training_loss = 0.21189476773142815\n",
      "epoch 8, number of batches processed 1800 of 3821, training_loss = 0.2271548455953598\n",
      "epoch 8, number of batches processed 1900 of 3821, training_loss = 0.2190286111086607\n",
      "epoch 8, number of batches processed 2000 of 3821, training_loss = 0.21185642577707767\n",
      "epoch 8, number of batches processed 2100 of 3821, training_loss = 0.2327359203249216\n",
      "epoch 8, number of batches processed 2200 of 3821, training_loss = 0.2102372492849827\n",
      "epoch 8, number of batches processed 2300 of 3821, training_loss = 0.22166649296879767\n",
      "epoch 8, number of batches processed 2400 of 3821, training_loss = 0.2301584067195654\n",
      "epoch 8, number of batches processed 2500 of 3821, training_loss = 0.22725847862660886\n",
      "epoch 8, number of batches processed 2600 of 3821, training_loss = 0.2210669980943203\n",
      "epoch 8, number of batches processed 2700 of 3821, training_loss = 0.22446761801838874\n",
      "epoch 8, number of batches processed 2800 of 3821, training_loss = 0.22391518600285054\n",
      "epoch 8, number of batches processed 2900 of 3821, training_loss = 0.21453056447207927\n",
      "epoch 8, number of batches processed 3000 of 3821, training_loss = 0.2149185112863779\n",
      "epoch 8, number of batches processed 3100 of 3821, training_loss = 0.21565923251211644\n",
      "epoch 8, number of batches processed 3200 of 3821, training_loss = 0.21663760162889958\n",
      "epoch 8, number of batches processed 3300 of 3821, training_loss = 0.20904441595077514\n",
      "epoch 8, number of batches processed 3400 of 3821, training_loss = 0.2284868973493576\n",
      "epoch 8, number of batches processed 3500 of 3821, training_loss = 0.2254924890398979\n",
      "epoch 8, number of batches processed 3600 of 3821, training_loss = 0.21520389355719088\n",
      "epoch 8, number of batches processed 3700 of 3821, training_loss = 0.215555030554533\n",
      "epoch 8, number of batches processed 3800 of 3821, training_loss = 0.24045978233218193\n",
      "0 0.846\n",
      "acc epoch8 0.8500833333333333\n",
      "epoch 9, number of batches processed 100 of 3821, training_loss = 0.1961883021891117\n",
      "epoch 9, number of batches processed 200 of 3821, training_loss = 0.19517613887786867\n",
      "epoch 9, number of batches processed 300 of 3821, training_loss = 0.17318954162299632\n",
      "epoch 9, number of batches processed 400 of 3821, training_loss = 0.18756772324442864\n",
      "epoch 9, number of batches processed 500 of 3821, training_loss = 0.18010588236153124\n",
      "epoch 9, number of batches processed 600 of 3821, training_loss = 0.17937978446483613\n",
      "epoch 9, number of batches processed 700 of 3821, training_loss = 0.20510146588087083\n",
      "epoch 9, number of batches processed 800 of 3821, training_loss = 0.18636148668825625\n",
      "epoch 9, number of batches processed 900 of 3821, training_loss = 0.18313823118805886\n",
      "epoch 9, number of batches processed 1000 of 3821, training_loss = 0.19511224322021006\n",
      "epoch 9, number of batches processed 1100 of 3821, training_loss = 0.19191623650491238\n",
      "epoch 9, number of batches processed 1200 of 3821, training_loss = 0.19038531742990017\n",
      "epoch 9, number of batches processed 1300 of 3821, training_loss = 0.18702497370541096\n",
      "epoch 9, number of batches processed 1400 of 3821, training_loss = 0.16990531720221042\n",
      "epoch 9, number of batches processed 1500 of 3821, training_loss = 0.19444334648549558\n",
      "epoch 9, number of batches processed 1600 of 3821, training_loss = 0.20802113629877567\n",
      "epoch 9, number of batches processed 1700 of 3821, training_loss = 0.19658363342285157\n",
      "epoch 9, number of batches processed 1800 of 3821, training_loss = 0.18511974260210992\n",
      "epoch 9, number of batches processed 1900 of 3821, training_loss = 0.2037626913189888\n",
      "epoch 9, number of batches processed 2000 of 3821, training_loss = 0.17773485720157622\n",
      "epoch 9, number of batches processed 2100 of 3821, training_loss = 0.19464550882577897\n",
      "epoch 9, number of batches processed 2200 of 3821, training_loss = 0.20276339754462241\n",
      "epoch 9, number of batches processed 2300 of 3821, training_loss = 0.18788059443235397\n",
      "epoch 9, number of batches processed 2400 of 3821, training_loss = 0.20033065043389797\n",
      "epoch 9, number of batches processed 2500 of 3821, training_loss = 0.19033355005085467\n",
      "epoch 9, number of batches processed 2600 of 3821, training_loss = 0.20087702557444573\n",
      "epoch 9, number of batches processed 2700 of 3821, training_loss = 0.19574356719851493\n",
      "epoch 9, number of batches processed 2800 of 3821, training_loss = 0.1909248360991478\n",
      "epoch 9, number of batches processed 2900 of 3821, training_loss = 0.16807755120098591\n",
      "epoch 9, number of batches processed 3000 of 3821, training_loss = 0.19366069324314594\n",
      "epoch 9, number of batches processed 3100 of 3821, training_loss = 0.19134195581078528\n",
      "epoch 9, number of batches processed 3200 of 3821, training_loss = 0.18791226245462894\n",
      "epoch 9, number of batches processed 3300 of 3821, training_loss = 0.19850129276514053\n",
      "epoch 9, number of batches processed 3400 of 3821, training_loss = 0.1851040641963482\n",
      "epoch 9, number of batches processed 3500 of 3821, training_loss = 0.17354998350143433\n",
      "epoch 9, number of batches processed 3600 of 3821, training_loss = 0.18304600328207016\n",
      "epoch 9, number of batches processed 3700 of 3821, training_loss = 0.2000314197689295\n",
      "epoch 9, number of batches processed 3800 of 3821, training_loss = 0.1797929085046053\n",
      "0 0.848\n",
      "acc epoch9 0.8555\n",
      "epoch 10, number of batches processed 100 of 3821, training_loss = 0.17025889478623868\n",
      "epoch 10, number of batches processed 200 of 3821, training_loss = 0.17917946241796018\n",
      "epoch 10, number of batches processed 300 of 3821, training_loss = 0.16389891907572746\n",
      "epoch 10, number of batches processed 400 of 3821, training_loss = 0.16280384123325348\n",
      "epoch 10, number of batches processed 500 of 3821, training_loss = 0.1775616481155157\n",
      "epoch 10, number of batches processed 600 of 3821, training_loss = 0.16952946685254575\n",
      "epoch 10, number of batches processed 700 of 3821, training_loss = 0.18497154742479324\n",
      "epoch 10, number of batches processed 800 of 3821, training_loss = 0.16936354838311674\n",
      "epoch 10, number of batches processed 900 of 3821, training_loss = 0.16156566806137562\n",
      "epoch 10, number of batches processed 1000 of 3821, training_loss = 0.16228900134563445\n",
      "epoch 10, number of batches processed 1100 of 3821, training_loss = 0.17762179590761662\n",
      "epoch 10, number of batches processed 1200 of 3821, training_loss = 0.1712716519832611\n",
      "epoch 10, number of batches processed 1300 of 3821, training_loss = 0.16321664556860924\n",
      "epoch 10, number of batches processed 1400 of 3821, training_loss = 0.17913817085325717\n",
      "epoch 10, number of batches processed 1500 of 3821, training_loss = 0.1739723438769579\n",
      "epoch 10, number of batches processed 1600 of 3821, training_loss = 0.16135796442627906\n",
      "epoch 10, number of batches processed 1700 of 3821, training_loss = 0.16753049522638322\n",
      "epoch 10, number of batches processed 1800 of 3821, training_loss = 0.15344016700983049\n",
      "epoch 10, number of batches processed 1900 of 3821, training_loss = 0.167009150236845\n",
      "epoch 10, number of batches processed 2000 of 3821, training_loss = 0.17066470332443714\n",
      "epoch 10, number of batches processed 2100 of 3821, training_loss = 0.1596199581772089\n",
      "epoch 10, number of batches processed 2200 of 3821, training_loss = 0.16144649989902973\n",
      "epoch 10, number of batches processed 2300 of 3821, training_loss = 0.16385029628872871\n",
      "epoch 10, number of batches processed 2400 of 3821, training_loss = 0.1603609585762024\n",
      "epoch 10, number of batches processed 2500 of 3821, training_loss = 0.16463331401348114\n",
      "epoch 10, number of batches processed 2600 of 3821, training_loss = 0.17638017781078816\n",
      "epoch 10, number of batches processed 2700 of 3821, training_loss = 0.1617274434119463\n",
      "epoch 10, number of batches processed 2800 of 3821, training_loss = 0.16567921124398707\n",
      "epoch 10, number of batches processed 2900 of 3821, training_loss = 0.17035098016262054\n",
      "epoch 10, number of batches processed 3000 of 3821, training_loss = 0.16113485962152482\n",
      "epoch 10, number of batches processed 3100 of 3821, training_loss = 0.15857909835875034\n",
      "epoch 10, number of batches processed 3200 of 3821, training_loss = 0.18247955225408077\n",
      "epoch 10, number of batches processed 3300 of 3821, training_loss = 0.14963393308222295\n",
      "epoch 10, number of batches processed 3400 of 3821, training_loss = 0.16723092049360275\n",
      "epoch 10, number of batches processed 3500 of 3821, training_loss = 0.1731234622746706\n",
      "epoch 10, number of batches processed 3600 of 3821, training_loss = 0.16330723501741887\n",
      "epoch 10, number of batches processed 3700 of 3821, training_loss = 0.16480200245976448\n",
      "epoch 10, number of batches processed 3800 of 3821, training_loss = 0.15837429516017437\n",
      "0 0.85\n",
      "acc epoch10 0.85775\n",
      "epoch 11, number of batches processed 100 of 3821, training_loss = 0.14935632154345513\n",
      "epoch 11, number of batches processed 200 of 3821, training_loss = 0.15470341354608536\n",
      "epoch 11, number of batches processed 300 of 3821, training_loss = 0.13969227746129037\n",
      "epoch 11, number of batches processed 400 of 3821, training_loss = 0.16483120195567608\n",
      "epoch 11, number of batches processed 500 of 3821, training_loss = 0.1409484077244997\n",
      "epoch 11, number of batches processed 600 of 3821, training_loss = 0.13707892365753652\n",
      "epoch 11, number of batches processed 700 of 3821, training_loss = 0.15082830667495728\n",
      "epoch 11, number of batches processed 800 of 3821, training_loss = 0.1526298826187849\n",
      "epoch 11, number of batches processed 900 of 3821, training_loss = 0.15142464242875575\n",
      "epoch 11, number of batches processed 1000 of 3821, training_loss = 0.14161425925791263\n",
      "epoch 11, number of batches processed 1100 of 3821, training_loss = 0.15434363186359407\n",
      "epoch 11, number of batches processed 1200 of 3821, training_loss = 0.14386077679693698\n",
      "epoch 11, number of batches processed 1300 of 3821, training_loss = 0.15325927026569844\n",
      "epoch 11, number of batches processed 1400 of 3821, training_loss = 0.16219711013138294\n",
      "epoch 11, number of batches processed 1500 of 3821, training_loss = 0.16293795965611935\n",
      "epoch 11, number of batches processed 1600 of 3821, training_loss = 0.1632119320333004\n",
      "epoch 11, number of batches processed 1700 of 3821, training_loss = 0.14038007944822312\n",
      "epoch 11, number of batches processed 1800 of 3821, training_loss = 0.1472032180428505\n",
      "epoch 11, number of batches processed 1900 of 3821, training_loss = 0.1450752353668213\n",
      "epoch 11, number of batches processed 2000 of 3821, training_loss = 0.14590623915195466\n",
      "epoch 11, number of batches processed 2100 of 3821, training_loss = 0.14349941700696944\n",
      "epoch 11, number of batches processed 2200 of 3821, training_loss = 0.14681532904505729\n",
      "epoch 11, number of batches processed 2300 of 3821, training_loss = 0.15450939394533633\n",
      "epoch 11, number of batches processed 2400 of 3821, training_loss = 0.14861455120146275\n",
      "epoch 11, number of batches processed 2500 of 3821, training_loss = 0.13310629986226558\n",
      "epoch 11, number of batches processed 2600 of 3821, training_loss = 0.14249598175287248\n",
      "epoch 11, number of batches processed 2700 of 3821, training_loss = 0.15230894863605499\n",
      "epoch 11, number of batches processed 2800 of 3821, training_loss = 0.15839272029697896\n",
      "epoch 11, number of batches processed 2900 of 3821, training_loss = 0.15393364369869234\n",
      "epoch 11, number of batches processed 3000 of 3821, training_loss = 0.14492106288671494\n",
      "epoch 11, number of batches processed 3100 of 3821, training_loss = 0.13342099234461785\n",
      "epoch 11, number of batches processed 3200 of 3821, training_loss = 0.14374282225966453\n",
      "epoch 11, number of batches processed 3300 of 3821, training_loss = 0.1683119137585163\n",
      "epoch 11, number of batches processed 3400 of 3821, training_loss = 0.13539055846631526\n",
      "epoch 11, number of batches processed 3500 of 3821, training_loss = 0.14877014249563217\n",
      "epoch 11, number of batches processed 3600 of 3821, training_loss = 0.1631387749314308\n",
      "epoch 11, number of batches processed 3700 of 3821, training_loss = 0.1424562169611454\n",
      "epoch 11, number of batches processed 3800 of 3821, training_loss = 0.15140216022729874\n",
      "0 0.852\n",
      "acc epoch11 0.86125\n",
      "epoch 12, number of batches processed 100 of 3821, training_loss = 0.14036805037409067\n",
      "epoch 12, number of batches processed 200 of 3821, training_loss = 0.12290011789649725\n",
      "epoch 12, number of batches processed 300 of 3821, training_loss = 0.14026864975690842\n",
      "epoch 12, number of batches processed 400 of 3821, training_loss = 0.13719640225172042\n",
      "epoch 12, number of batches processed 500 of 3821, training_loss = 0.13498651824891567\n",
      "epoch 12, number of batches processed 600 of 3821, training_loss = 0.13902812629938124\n",
      "epoch 12, number of batches processed 700 of 3821, training_loss = 0.14274822376668453\n",
      "epoch 12, number of batches processed 800 of 3821, training_loss = 0.14029228501021862\n",
      "epoch 12, number of batches processed 900 of 3821, training_loss = 0.13392538852989674\n",
      "epoch 12, number of batches processed 1000 of 3821, training_loss = 0.14904935620725154\n",
      "epoch 12, number of batches processed 1100 of 3821, training_loss = 0.13474660173058509\n",
      "epoch 12, number of batches processed 1200 of 3821, training_loss = 0.14380473248660564\n",
      "epoch 12, number of batches processed 1300 of 3821, training_loss = 0.12503233969211577\n",
      "epoch 12, number of batches processed 1400 of 3821, training_loss = 0.13859273292124272\n",
      "epoch 12, number of batches processed 1500 of 3821, training_loss = 0.13594392724335194\n",
      "epoch 12, number of batches processed 1600 of 3821, training_loss = 0.14244855925440789\n",
      "epoch 12, number of batches processed 1700 of 3821, training_loss = 0.14264096044003963\n",
      "epoch 12, number of batches processed 1800 of 3821, training_loss = 0.13929671656340362\n",
      "epoch 12, number of batches processed 1900 of 3821, training_loss = 0.14542786791920662\n",
      "epoch 12, number of batches processed 2000 of 3821, training_loss = 0.12978127874433995\n",
      "epoch 12, number of batches processed 2100 of 3821, training_loss = 0.12701237134635449\n",
      "epoch 12, number of batches processed 2200 of 3821, training_loss = 0.12805343329906463\n",
      "epoch 12, number of batches processed 2300 of 3821, training_loss = 0.13329081907868384\n",
      "epoch 12, number of batches processed 2400 of 3821, training_loss = 0.13847890235483645\n",
      "epoch 12, number of batches processed 2500 of 3821, training_loss = 0.13596748895943164\n",
      "epoch 12, number of batches processed 2600 of 3821, training_loss = 0.12566497668623924\n",
      "epoch 12, number of batches processed 2700 of 3821, training_loss = 0.13776094309985637\n",
      "epoch 12, number of batches processed 2800 of 3821, training_loss = 0.13836722023785114\n",
      "epoch 12, number of batches processed 2900 of 3821, training_loss = 0.12587865740060805\n",
      "epoch 12, number of batches processed 3000 of 3821, training_loss = 0.13379275381565095\n",
      "epoch 12, number of batches processed 3100 of 3821, training_loss = 0.1348977265506983\n",
      "epoch 12, number of batches processed 3200 of 3821, training_loss = 0.13576990842819214\n",
      "epoch 12, number of batches processed 3300 of 3821, training_loss = 0.13058193899691106\n",
      "epoch 12, number of batches processed 3400 of 3821, training_loss = 0.12763634033501148\n",
      "epoch 12, number of batches processed 3500 of 3821, training_loss = 0.13565778076648713\n",
      "epoch 12, number of batches processed 3600 of 3821, training_loss = 0.145010152682662\n",
      "epoch 12, number of batches processed 3700 of 3821, training_loss = 0.13193162027746438\n",
      "epoch 12, number of batches processed 3800 of 3821, training_loss = 0.1325336903333664\n",
      "0 0.854\n",
      "acc epoch12 0.8631666666666666\n",
      "epoch 13, number of batches processed 100 of 3821, training_loss = 0.12202558033168316\n",
      "epoch 13, number of batches processed 200 of 3821, training_loss = 0.12469163477420807\n",
      "epoch 13, number of batches processed 300 of 3821, training_loss = 0.11589473184198142\n",
      "epoch 13, number of batches processed 400 of 3821, training_loss = 0.11384976133704186\n",
      "epoch 13, number of batches processed 500 of 3821, training_loss = 0.1392847489565611\n",
      "epoch 13, number of batches processed 600 of 3821, training_loss = 0.1263267148286104\n",
      "epoch 13, number of batches processed 700 of 3821, training_loss = 0.12033420763909816\n",
      "epoch 13, number of batches processed 800 of 3821, training_loss = 0.129584525488317\n",
      "epoch 13, number of batches processed 900 of 3821, training_loss = 0.11245062801986933\n",
      "epoch 13, number of batches processed 1000 of 3821, training_loss = 0.13835481829941274\n",
      "epoch 13, number of batches processed 1100 of 3821, training_loss = 0.12322577692568303\n",
      "epoch 13, number of batches processed 1200 of 3821, training_loss = 0.11851986892521381\n",
      "epoch 13, number of batches processed 1300 of 3821, training_loss = 0.12584235869348048\n",
      "epoch 13, number of batches processed 1400 of 3821, training_loss = 0.130981782451272\n",
      "epoch 13, number of batches processed 1500 of 3821, training_loss = 0.12838500522077084\n",
      "epoch 13, number of batches processed 1600 of 3821, training_loss = 0.12319453619420528\n",
      "epoch 13, number of batches processed 1700 of 3821, training_loss = 0.125966217815876\n",
      "epoch 13, number of batches processed 1800 of 3821, training_loss = 0.11566576741635799\n",
      "epoch 13, number of batches processed 1900 of 3821, training_loss = 0.12793389782309533\n",
      "epoch 13, number of batches processed 2000 of 3821, training_loss = 0.12336525786668062\n",
      "epoch 13, number of batches processed 2100 of 3821, training_loss = 0.12415016304701566\n",
      "epoch 13, number of batches processed 2200 of 3821, training_loss = 0.10881250768899918\n",
      "epoch 13, number of batches processed 2300 of 3821, training_loss = 0.12740208111703397\n",
      "epoch 13, number of batches processed 2400 of 3821, training_loss = 0.13034840390086175\n",
      "epoch 13, number of batches processed 2500 of 3821, training_loss = 0.11797622982412577\n",
      "epoch 13, number of batches processed 2600 of 3821, training_loss = 0.13449784845113755\n",
      "epoch 13, number of batches processed 2700 of 3821, training_loss = 0.1342187797278166\n",
      "epoch 13, number of batches processed 2800 of 3821, training_loss = 0.12538996763527394\n",
      "epoch 13, number of batches processed 2900 of 3821, training_loss = 0.11422465674579144\n",
      "epoch 13, number of batches processed 3000 of 3821, training_loss = 0.12151673026382923\n",
      "epoch 13, number of batches processed 3100 of 3821, training_loss = 0.12648192144930362\n",
      "epoch 13, number of batches processed 3200 of 3821, training_loss = 0.12603704657405615\n",
      "epoch 13, number of batches processed 3300 of 3821, training_loss = 0.12476554173976183\n",
      "epoch 13, number of batches processed 3400 of 3821, training_loss = 0.1322144278883934\n",
      "epoch 13, number of batches processed 3500 of 3821, training_loss = 0.1277878589183092\n",
      "epoch 13, number of batches processed 3600 of 3821, training_loss = 0.13058259919285775\n",
      "epoch 13, number of batches processed 3700 of 3821, training_loss = 0.12763176187872888\n",
      "epoch 13, number of batches processed 3800 of 3821, training_loss = 0.11862461887300015\n",
      "0 0.855\n",
      "acc epoch13 0.8646666666666667\n",
      "epoch 14, number of batches processed 100 of 3821, training_loss = 0.11857654746621847\n",
      "epoch 14, number of batches processed 200 of 3821, training_loss = 0.10825724348425865\n",
      "epoch 14, number of batches processed 300 of 3821, training_loss = 0.10394399061799049\n",
      "epoch 14, number of batches processed 400 of 3821, training_loss = 0.1137485694885254\n",
      "epoch 14, number of batches processed 500 of 3821, training_loss = 0.11929285436868668\n",
      "epoch 14, number of batches processed 600 of 3821, training_loss = 0.11770732775330543\n",
      "epoch 14, number of batches processed 700 of 3821, training_loss = 0.1280856701731682\n",
      "epoch 14, number of batches processed 800 of 3821, training_loss = 0.11731434367597103\n",
      "epoch 14, number of batches processed 900 of 3821, training_loss = 0.11412406556308269\n",
      "epoch 14, number of batches processed 1000 of 3821, training_loss = 0.11968276564031839\n",
      "epoch 14, number of batches processed 1100 of 3821, training_loss = 0.12622266754508019\n",
      "epoch 14, number of batches processed 1200 of 3821, training_loss = 0.12676298782229423\n",
      "epoch 14, number of batches processed 1300 of 3821, training_loss = 0.11563658099621535\n",
      "epoch 14, number of batches processed 1400 of 3821, training_loss = 0.11773186858743429\n",
      "epoch 14, number of batches processed 1500 of 3821, training_loss = 0.11058594569563866\n",
      "epoch 14, number of batches processed 1600 of 3821, training_loss = 0.11593098439276218\n",
      "epoch 14, number of batches processed 1700 of 3821, training_loss = 0.11119653418660164\n",
      "epoch 14, number of batches processed 1800 of 3821, training_loss = 0.11249817840754986\n",
      "epoch 14, number of batches processed 1900 of 3821, training_loss = 0.11951112810522319\n",
      "epoch 14, number of batches processed 2000 of 3821, training_loss = 0.12069652516394853\n",
      "epoch 14, number of batches processed 2100 of 3821, training_loss = 0.11990292757749557\n",
      "epoch 14, number of batches processed 2200 of 3821, training_loss = 0.10989301521331071\n",
      "epoch 14, number of batches processed 2300 of 3821, training_loss = 0.11853475887328387\n",
      "epoch 14, number of batches processed 2400 of 3821, training_loss = 0.12113359205424785\n",
      "epoch 14, number of batches processed 2500 of 3821, training_loss = 0.10324159733951092\n",
      "epoch 14, number of batches processed 2600 of 3821, training_loss = 0.11789562836289406\n",
      "epoch 14, number of batches processed 2700 of 3821, training_loss = 0.12811467390507458\n",
      "epoch 14, number of batches processed 2800 of 3821, training_loss = 0.1314668871462345\n",
      "epoch 14, number of batches processed 2900 of 3821, training_loss = 0.11676372200250626\n",
      "epoch 14, number of batches processed 3000 of 3821, training_loss = 0.11077386591583491\n",
      "epoch 14, number of batches processed 3100 of 3821, training_loss = 0.11153463985770941\n",
      "epoch 14, number of batches processed 3200 of 3821, training_loss = 0.10803455673158169\n",
      "epoch 14, number of batches processed 3300 of 3821, training_loss = 0.113565673828125\n",
      "epoch 14, number of batches processed 3400 of 3821, training_loss = 0.12011982671916485\n",
      "epoch 14, number of batches processed 3500 of 3821, training_loss = 0.11367911513894796\n",
      "epoch 14, number of batches processed 3600 of 3821, training_loss = 0.10953706864267587\n",
      "epoch 14, number of batches processed 3700 of 3821, training_loss = 0.11815584450960159\n",
      "epoch 14, number of batches processed 3800 of 3821, training_loss = 0.12222484439611435\n",
      "0 0.863\n",
      "acc epoch14 0.86725\n",
      "epoch 15, number of batches processed 100 of 3821, training_loss = 0.11000387284904718\n",
      "epoch 15, number of batches processed 200 of 3821, training_loss = 0.10774192236363887\n",
      "epoch 15, number of batches processed 300 of 3821, training_loss = 0.10444899946451187\n",
      "epoch 15, number of batches processed 400 of 3821, training_loss = 0.10896083470433951\n",
      "epoch 15, number of batches processed 500 of 3821, training_loss = 0.10818364553153514\n",
      "epoch 15, number of batches processed 600 of 3821, training_loss = 0.11071717817336321\n",
      "epoch 15, number of batches processed 700 of 3821, training_loss = 0.09782315887510777\n",
      "epoch 15, number of batches processed 800 of 3821, training_loss = 0.10505880493670702\n",
      "epoch 15, number of batches processed 900 of 3821, training_loss = 0.10281482797116041\n",
      "epoch 15, number of batches processed 1000 of 3821, training_loss = 0.10971633616834879\n",
      "epoch 15, number of batches processed 1100 of 3821, training_loss = 0.1063428258150816\n",
      "epoch 15, number of batches processed 1200 of 3821, training_loss = 0.1134782586991787\n",
      "epoch 15, number of batches processed 1300 of 3821, training_loss = 0.10874022323638201\n",
      "epoch 15, number of batches processed 1400 of 3821, training_loss = 0.10960314616560936\n",
      "epoch 15, number of batches processed 1500 of 3821, training_loss = 0.10152774885296821\n",
      "epoch 15, number of batches processed 1600 of 3821, training_loss = 0.10463555812835694\n",
      "epoch 15, number of batches processed 1700 of 3821, training_loss = 0.10999491892755031\n",
      "epoch 15, number of batches processed 1800 of 3821, training_loss = 0.10590446442365646\n",
      "epoch 15, number of batches processed 1900 of 3821, training_loss = 0.11772321090102196\n",
      "epoch 15, number of batches processed 2000 of 3821, training_loss = 0.11775432631373406\n",
      "epoch 15, number of batches processed 2100 of 3821, training_loss = 0.11030832525342703\n",
      "epoch 15, number of batches processed 2200 of 3821, training_loss = 0.10701486233621836\n",
      "epoch 15, number of batches processed 2300 of 3821, training_loss = 0.10330545067787171\n",
      "epoch 15, number of batches processed 2400 of 3821, training_loss = 0.1141593549400568\n",
      "epoch 15, number of batches processed 2500 of 3821, training_loss = 0.1131785947456956\n",
      "epoch 15, number of batches processed 2600 of 3821, training_loss = 0.11407097142189741\n",
      "epoch 15, number of batches processed 2700 of 3821, training_loss = 0.11196833029389382\n",
      "epoch 15, number of batches processed 2800 of 3821, training_loss = 0.11277075961232186\n",
      "epoch 15, number of batches processed 2900 of 3821, training_loss = 0.12033271715044976\n",
      "epoch 15, number of batches processed 3000 of 3821, training_loss = 0.10551730491220951\n",
      "epoch 15, number of batches processed 3100 of 3821, training_loss = 0.11932175736874343\n",
      "epoch 15, number of batches processed 3200 of 3821, training_loss = 0.11029019691050053\n",
      "epoch 15, number of batches processed 3300 of 3821, training_loss = 0.10270026285201311\n",
      "epoch 15, number of batches processed 3400 of 3821, training_loss = 0.11196476258337498\n",
      "epoch 15, number of batches processed 3500 of 3821, training_loss = 0.10584229454398156\n",
      "epoch 15, number of batches processed 3600 of 3821, training_loss = 0.10799973648041487\n",
      "epoch 15, number of batches processed 3700 of 3821, training_loss = 0.11926804475486279\n",
      "epoch 15, number of batches processed 3800 of 3821, training_loss = 0.10356154341250658\n",
      "0 0.865\n",
      "acc epoch15 0.8686666666666667\n",
      "epoch 16, number of batches processed 100 of 3821, training_loss = 0.10425353549420834\n",
      "epoch 16, number of batches processed 200 of 3821, training_loss = 0.10514383088797331\n",
      "epoch 16, number of batches processed 300 of 3821, training_loss = 0.09809025883674621\n",
      "epoch 16, number of batches processed 400 of 3821, training_loss = 0.1047770482301712\n",
      "epoch 16, number of batches processed 500 of 3821, training_loss = 0.09770054027438163\n",
      "epoch 16, number of batches processed 600 of 3821, training_loss = 0.10066498428583145\n",
      "epoch 16, number of batches processed 700 of 3821, training_loss = 0.10965627200901508\n",
      "epoch 16, number of batches processed 800 of 3821, training_loss = 0.1062356984615326\n",
      "epoch 16, number of batches processed 900 of 3821, training_loss = 0.09635976120829583\n",
      "epoch 16, number of batches processed 1000 of 3821, training_loss = 0.10498924259096384\n",
      "epoch 16, number of batches processed 1100 of 3821, training_loss = 0.10293450895696879\n",
      "epoch 16, number of batches processed 1200 of 3821, training_loss = 0.09968432400375604\n",
      "epoch 16, number of batches processed 1300 of 3821, training_loss = 0.10253067180514336\n",
      "epoch 16, number of batches processed 1400 of 3821, training_loss = 0.10742887619882822\n",
      "epoch 16, number of batches processed 1500 of 3821, training_loss = 0.09776192117482424\n",
      "epoch 16, number of batches processed 1600 of 3821, training_loss = 0.09830282665789128\n",
      "epoch 16, number of batches processed 1700 of 3821, training_loss = 0.11094846289604902\n",
      "epoch 16, number of batches processed 1800 of 3821, training_loss = 0.10093190900981426\n",
      "epoch 16, number of batches processed 1900 of 3821, training_loss = 0.09834230177104474\n",
      "epoch 16, number of batches processed 2000 of 3821, training_loss = 0.11039390429854393\n",
      "epoch 16, number of batches processed 2100 of 3821, training_loss = 0.10505847662687301\n",
      "epoch 16, number of batches processed 2200 of 3821, training_loss = 0.10650114957243204\n",
      "epoch 16, number of batches processed 2300 of 3821, training_loss = 0.101624724753201\n",
      "epoch 16, number of batches processed 2400 of 3821, training_loss = 0.10053858391940594\n",
      "epoch 16, number of batches processed 2500 of 3821, training_loss = 0.10921764008700847\n",
      "epoch 16, number of batches processed 2600 of 3821, training_loss = 0.09481670632958412\n",
      "epoch 16, number of batches processed 2700 of 3821, training_loss = 0.10252911765128374\n",
      "epoch 16, number of batches processed 2800 of 3821, training_loss = 0.11148013211786748\n",
      "epoch 16, number of batches processed 2900 of 3821, training_loss = 0.09378024600446225\n",
      "epoch 16, number of batches processed 3000 of 3821, training_loss = 0.09881433136761189\n",
      "epoch 16, number of batches processed 3100 of 3821, training_loss = 0.1133958126604557\n",
      "epoch 16, number of batches processed 3200 of 3821, training_loss = 0.10003518979996442\n",
      "epoch 16, number of batches processed 3300 of 3821, training_loss = 0.10281997431069613\n",
      "epoch 16, number of batches processed 3400 of 3821, training_loss = 0.10638887334614992\n",
      "epoch 16, number of batches processed 3500 of 3821, training_loss = 0.1075820530578494\n",
      "epoch 16, number of batches processed 3600 of 3821, training_loss = 0.11189409464597702\n",
      "epoch 16, number of batches processed 3700 of 3821, training_loss = 0.10788421086966991\n",
      "epoch 16, number of batches processed 3800 of 3821, training_loss = 0.10403499674052\n",
      "0 0.87\n",
      "acc epoch16 0.8703333333333333\n",
      "epoch 17, number of batches processed 100 of 3821, training_loss = 0.09859266471117735\n",
      "epoch 17, number of batches processed 200 of 3821, training_loss = 0.09657282058149576\n",
      "epoch 17, number of batches processed 300 of 3821, training_loss = 0.09581120729446412\n",
      "epoch 17, number of batches processed 400 of 3821, training_loss = 0.09046767111867667\n",
      "epoch 17, number of batches processed 500 of 3821, training_loss = 0.09772095084190369\n",
      "epoch 17, number of batches processed 600 of 3821, training_loss = 0.09258726757019758\n",
      "epoch 17, number of batches processed 700 of 3821, training_loss = 0.10626245453953743\n",
      "epoch 17, number of batches processed 800 of 3821, training_loss = 0.09803089655935765\n",
      "epoch 17, number of batches processed 900 of 3821, training_loss = 0.09308209408074618\n",
      "epoch 17, number of batches processed 1000 of 3821, training_loss = 0.10439617730677128\n",
      "epoch 17, number of batches processed 1100 of 3821, training_loss = 0.11133747700601816\n",
      "epoch 17, number of batches processed 1200 of 3821, training_loss = 0.09813896741718053\n",
      "epoch 17, number of batches processed 1300 of 3821, training_loss = 0.09253529131412506\n",
      "epoch 17, number of batches processed 1400 of 3821, training_loss = 0.0993877286463976\n",
      "epoch 17, number of batches processed 1500 of 3821, training_loss = 0.10481404673308134\n",
      "epoch 17, number of batches processed 1600 of 3821, training_loss = 0.10128878589719534\n",
      "epoch 17, number of batches processed 1700 of 3821, training_loss = 0.10269567377865314\n",
      "epoch 17, number of batches processed 1800 of 3821, training_loss = 0.09544688951224088\n",
      "epoch 17, number of batches processed 1900 of 3821, training_loss = 0.10103601627051831\n",
      "epoch 17, number of batches processed 2000 of 3821, training_loss = 0.09895152255892753\n",
      "epoch 17, number of batches processed 2100 of 3821, training_loss = 0.0899934495612979\n",
      "epoch 17, number of batches processed 2200 of 3821, training_loss = 0.09908507604151964\n",
      "epoch 17, number of batches processed 2300 of 3821, training_loss = 0.0917392209917307\n",
      "epoch 17, number of batches processed 2400 of 3821, training_loss = 0.09420708235353231\n",
      "epoch 17, number of batches processed 2500 of 3821, training_loss = 0.10269163250923156\n",
      "epoch 17, number of batches processed 2600 of 3821, training_loss = 0.09616515137255192\n",
      "epoch 17, number of batches processed 2700 of 3821, training_loss = 0.1010885401815176\n",
      "epoch 17, number of batches processed 2800 of 3821, training_loss = 0.09628088112920523\n",
      "epoch 17, number of batches processed 2900 of 3821, training_loss = 0.09779782384634018\n",
      "epoch 17, number of batches processed 3000 of 3821, training_loss = 0.09508921377360821\n",
      "epoch 17, number of batches processed 3100 of 3821, training_loss = 0.11122746307402849\n",
      "epoch 17, number of batches processed 3200 of 3821, training_loss = 0.10086498662829399\n",
      "epoch 17, number of batches processed 3300 of 3821, training_loss = 0.09772475749254227\n",
      "epoch 17, number of batches processed 3400 of 3821, training_loss = 0.09731104794889689\n",
      "epoch 17, number of batches processed 3500 of 3821, training_loss = 0.10012766979634762\n",
      "epoch 17, number of batches processed 3600 of 3821, training_loss = 0.09343979831784964\n",
      "epoch 17, number of batches processed 3700 of 3821, training_loss = 0.10359647896140814\n",
      "epoch 17, number of batches processed 3800 of 3821, training_loss = 0.08718878254294396\n",
      "0 0.867\n",
      "acc epoch17 0.8720833333333333\n",
      "epoch 18, number of batches processed 100 of 3821, training_loss = 0.09023424431681633\n",
      "epoch 18, number of batches processed 200 of 3821, training_loss = 0.08838261175900698\n",
      "epoch 18, number of batches processed 300 of 3821, training_loss = 0.09473666090518236\n",
      "epoch 18, number of batches processed 400 of 3821, training_loss = 0.08954886212944985\n",
      "epoch 18, number of batches processed 500 of 3821, training_loss = 0.08882629439234734\n",
      "epoch 18, number of batches processed 600 of 3821, training_loss = 0.0831418413668871\n",
      "epoch 18, number of batches processed 700 of 3821, training_loss = 0.08371355257928371\n",
      "epoch 18, number of batches processed 800 of 3821, training_loss = 0.09370373994112015\n",
      "epoch 18, number of batches processed 900 of 3821, training_loss = 0.10005349699407816\n",
      "epoch 18, number of batches processed 1000 of 3821, training_loss = 0.09314303167164326\n",
      "epoch 18, number of batches processed 1100 of 3821, training_loss = 0.08863450683653355\n",
      "epoch 18, number of batches processed 1200 of 3821, training_loss = 0.09322711650282145\n",
      "epoch 18, number of batches processed 1300 of 3821, training_loss = 0.09849919188767671\n",
      "epoch 18, number of batches processed 1400 of 3821, training_loss = 0.0915764544159174\n",
      "epoch 18, number of batches processed 1500 of 3821, training_loss = 0.08970568019896746\n",
      "epoch 18, number of batches processed 1600 of 3821, training_loss = 0.0932777852192521\n",
      "epoch 18, number of batches processed 1700 of 3821, training_loss = 0.09135617800056935\n",
      "epoch 18, number of batches processed 1800 of 3821, training_loss = 0.09990486294031144\n",
      "epoch 18, number of batches processed 1900 of 3821, training_loss = 0.08944666057825089\n",
      "epoch 18, number of batches processed 2000 of 3821, training_loss = 0.09420173451304435\n",
      "epoch 18, number of batches processed 2100 of 3821, training_loss = 0.09732010122388601\n",
      "epoch 18, number of batches processed 2200 of 3821, training_loss = 0.10147228632122278\n",
      "epoch 18, number of batches processed 2300 of 3821, training_loss = 0.08894965920597314\n",
      "epoch 18, number of batches processed 2400 of 3821, training_loss = 0.09337192106992007\n",
      "epoch 18, number of batches processed 2500 of 3821, training_loss = 0.09720320921391248\n",
      "epoch 18, number of batches processed 2600 of 3821, training_loss = 0.09154968988150358\n",
      "epoch 18, number of batches processed 2700 of 3821, training_loss = 0.10160362489521503\n",
      "epoch 18, number of batches processed 2800 of 3821, training_loss = 0.09674635369330645\n",
      "epoch 18, number of batches processed 2900 of 3821, training_loss = 0.10230678748339414\n",
      "epoch 18, number of batches processed 3000 of 3821, training_loss = 0.097093459777534\n",
      "epoch 18, number of batches processed 3100 of 3821, training_loss = 0.09892716575413943\n",
      "epoch 18, number of batches processed 3200 of 3821, training_loss = 0.08650502692908049\n",
      "epoch 18, number of batches processed 3300 of 3821, training_loss = 0.08870576772838831\n",
      "epoch 18, number of batches processed 3400 of 3821, training_loss = 0.10053658626973629\n",
      "epoch 18, number of batches processed 3500 of 3821, training_loss = 0.09751980546861887\n",
      "epoch 18, number of batches processed 3600 of 3821, training_loss = 0.09413860619068146\n",
      "epoch 18, number of batches processed 3700 of 3821, training_loss = 0.0902470987290144\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ajrfhp/nlp/learning/word2vec.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ajrfhp/nlp/learning/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(word_to_vec\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ajrfhp/nlp/learning/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39meval\u001b[39m(word_to_vec, eval_dataloader)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ajrfhp/nlp/learning/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m train(word_to_vec, train_dataloader, eval_dataloader, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ajrfhp/nlp/learning/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39meval\u001b[39m(word_to_vec, eval_dataloader)\n",
      "\u001b[1;32m/home/ajrfhp/nlp/learning/word2vec.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, logging_interval, epochs)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ajrfhp/nlp/learning/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ajrfhp/nlp/learning/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ajrfhp/nlp/learning/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ajrfhp/nlp/learning/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         inputs, outputs \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m'\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ajrfhp/nlp/learning/word2vec.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/datasets/arrow_dataset.py:2165\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2164\u001b[0m     \u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2165\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(\n\u001b[1;32m   2166\u001b[0m         key,\n\u001b[1;32m   2167\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/datasets/arrow_dataset.py:2150\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, decoded\u001b[39m=\u001b[39mdecoded, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2149\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, key, indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 2150\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2151\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39;49mformatter, format_columns\u001b[39m=\u001b[39;49mformat_columns, output_all_columns\u001b[39m=\u001b[39;49moutput_all_columns\n\u001b[1;32m   2152\u001b[0m )\n\u001b[1;32m   2153\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/datasets/formatting/formatting.py:532\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    530\u001b[0m python_formatter \u001b[39m=\u001b[39m PythonFormatter(features\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    531\u001b[0m \u001b[39mif\u001b[39;00m format_columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 532\u001b[0m     \u001b[39mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[39m=\u001b[39;49mquery_type)\n\u001b[1;32m    533\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    534\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/datasets/formatting/formatting.py:281\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable, query_type: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 281\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_row(pa_table)\n\u001b[1;32m    282\u001b[0m     \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    283\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py:57\u001b[0m, in \u001b[0;36mTorchFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_row\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m     row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy_arrow_extractor()\u001b[39m.\u001b[39;49mextract_row(pa_table)\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecursive_tensorize(row)\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/datasets/formatting/formatting.py:154\u001b[0m, in \u001b[0;36mNumpyArrowExtractor.extract_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_row\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[39mreturn\u001b[39;00m _unnest(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_batch(pa_table))\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/datasets/formatting/formatting.py:160\u001b[0m, in \u001b[0;36mNumpyArrowExtractor.extract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_batch\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m {col: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_arrow_array_to_numpy(pa_table[col]) \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m pa_table\u001b[39m.\u001b[39mcolumn_names}\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/datasets/formatting/formatting.py:160\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_batch\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m {col: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_arrow_array_to_numpy(pa_table[col]) \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m pa_table\u001b[39m.\u001b[39mcolumn_names}\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/datasets/formatting/formatting.py:181\u001b[0m, in \u001b[0;36mNumpyArrowExtractor._arrow_array_to_numpy\u001b[0;34m(self, pa_array)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m         zero_copy_only \u001b[39m=\u001b[39m _is_zero_copy_only(pa_array\u001b[39m.\u001b[39mtype) \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    179\u001b[0m             \u001b[39mnot\u001b[39;00m _is_array_with_nulls(chunk) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m pa_array\u001b[39m.\u001b[39mchunks\n\u001b[1;32m    180\u001b[0m         )\n\u001b[0;32m--> 181\u001b[0m         array: List \u001b[39m=\u001b[39m [\n\u001b[1;32m    182\u001b[0m             row \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m pa_array\u001b[39m.\u001b[39mchunks \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m chunk\u001b[39m.\u001b[39mto_numpy(zero_copy_only\u001b[39m=\u001b[39mzero_copy_only)\n\u001b[1;32m    183\u001b[0m         ]\n\u001b[1;32m    184\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(pa_array\u001b[39m.\u001b[39mtype, _ArrayXDExtensionType):\n\u001b[1;32m    186\u001b[0m         \u001b[39m# don't call to_pylist() to preserve dtype of the fixed-size array\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/reddevils_nlp/lib/python3.8/site-packages/datasets/formatting/formatting.py:182\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m         zero_copy_only \u001b[39m=\u001b[39m _is_zero_copy_only(pa_array\u001b[39m.\u001b[39mtype) \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    179\u001b[0m             \u001b[39mnot\u001b[39;00m _is_array_with_nulls(chunk) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m pa_array\u001b[39m.\u001b[39mchunks\n\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m         array: List \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 182\u001b[0m             row \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m pa_array\u001b[39m.\u001b[39mchunks \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m chunk\u001b[39m.\u001b[39;49mto_numpy(zero_copy_only\u001b[39m=\u001b[39;49mzero_copy_only)\n\u001b[1;32m    183\u001b[0m         ]\n\u001b[1;32m    184\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(pa_array\u001b[39m.\u001b[39mtype, _ArrayXDExtensionType):\n\u001b[1;32m    186\u001b[0m         \u001b[39m# don't call to_pylist() to preserve dtype of the fixed-size array\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, train_dataloader, test_dataloader, optimizer, logging_interval=100, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            inputs, outputs = batch['inputs'].to(device), batch['outputs'].to(device)\n",
    "            predictions = model.forward(inputs)\n",
    "            loss = nn.functional.binary_cross_entropy(predictions, outputs.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            if i and i%logging_interval == 0:\n",
    "                running_loss /= logging_interval\n",
    "                print(f'epoch {epoch}, number of batches processed {i} of {len(train_dataloader)}, training_loss = {running_loss}')\n",
    "                running_loss = 0\n",
    "        torch.save(word_to_vec.state_dict(), f'./word_to_vec{epoch}.pt')\n",
    "        print(f'acc epoch{epoch} {eval(model, test_dataloader)}')\n",
    "\n",
    "def eval(model, test_dataloder, logging_interval=100):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(test_dataloder):\n",
    "        inputs, outputs = batch['inputs'].to(device), batch['outputs'].to(device)\n",
    "        predictions = model.forward(inputs)\n",
    "        predictions[predictions.le(0.5)] = 0\n",
    "        predictions[predictions.ge(0.5)] = 1\n",
    "        correct += len(predictions[predictions == outputs])\n",
    "        total += len(outputs)\n",
    "        if i % logging_interval == 0:\n",
    "            print(i, correct/total)\n",
    "        if i > 10:\n",
    "            break\n",
    "    return correct/total\n",
    "\n",
    "        \n",
    "\n",
    "word_to_vec = WordToVec(tokenizer.vocab_size, 100).to(device)\n",
    "optimizer = optim.Adam(word_to_vec.parameters(), lr=1e-3)\n",
    "eval(word_to_vec, eval_dataloader)\n",
    "train(word_to_vec, train_dataloader, eval_dataloader, optimizer)\n",
    "eval(word_to_vec, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1522, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(0.1385, device='cuda:0', grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vector(word):\n",
    "    int_id = torch.tensor([tokenizer(word)['input_ids']]).to(device)\n",
    "    vector = word_to_vec.embedding.forward(int_id)[0][1]\n",
    "    return vector.reshape((-1, ))\n",
    "\n",
    "def get_vector_similarity(a, b):\n",
    "    num = a.dot(b)\n",
    "    den = torch.sqrt((a*a).sum() * (b*b).sum())\n",
    "    return num / den\n",
    "    \n",
    "get_vector_similarity( get_vector('computer'), get_vector('information')), get_vector_similarity(get_vector('computer'), get_vector('football'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vector_similarity(torch.tensor([1, 2, 3]), torch.tensor([-1, -2, -3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('reddevils_nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "207f4bc9e0c83dc0c8ddefb60bf2781db591dfbb838789bb02122c3833b8e815"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
